{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "id": "9-KbTD-Sae_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2Yq49j_aKNl"
      },
      "outputs": [],
      "source": [
        "#download the Fineweb-edu 10B dataset, tokenize and save to S3\n",
        "#adapted the code from here: https://github.com/karpathy/build-nanogpt/blob/master/fineweb.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "aws_access_key = \"\"\n",
        "aws_secret_key = \"\""
      ],
      "metadata": {
        "id": "qLlBKK8Oaqva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fineweb dataset"
      ],
      "metadata": {
        "id": "U_O1wLOHbWNX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken datasets tqdm"
      ],
      "metadata": {
        "id": "viiy8nOpblAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import multiprocessing as mp\n",
        "import numpy as np\n",
        "import tiktoken\n",
        "from datasets import load_dataset # pip install datasets\n",
        "from tqdm import tqdm # pip install tqdm"
      ],
      "metadata": {
        "id": "Csohff-nblON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nprocs = max(1, os.cpu_count()//2)\n",
        "nprocs"
      ],
      "metadata": {
        "id": "h0pq8ZSlblV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_dir = \"edu_fineweb10B\"\n",
        "remote_name = \"sample-10BT\" #\"CC-MAIN-2024-10\"#\n",
        "shard_size = int(1e8) # 100M tokens per shard, total of 100 shards\n",
        "print(shard_size)"
      ],
      "metadata": {
        "id": "VxQhtVcYblZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(local_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "75Db4_y_b7ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the dataset\n",
        "fw = load_dataset(\"HuggingFaceFW/fineweb-edu\", name=remote_name, streaming=True, split=\"train\")"
      ],
      "metadata": {
        "id": "IIjshmhPb7l_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init the tokenizer\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "eot = enc._special_tokens['<|endoftext|>'] # end of text token\n",
        "def tokenize(doc):\n",
        "    # tokenizes a single document and returns a numpy array of uint16 tokens\n",
        "    tokens = [eot] # the special <|endoftext|> token delimits all documents\n",
        "    tokens.extend(enc.encode_ordinary(doc[\"text\"]))\n",
        "    tokens_np = np.array(tokens)\n",
        "    assert (0 <= tokens_np).all() and (tokens_np < 2**16).all(), \"token dictionary too large for uint16\"\n",
        "    tokens_np_uint16 = tokens_np.astype(np.uint16)\n",
        "    return tokens_np_uint16"
      ],
      "metadata": {
        "id": "vJNWrM_sb7oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_datafile(filename, tokens_np):\n",
        "    np.save(filename, tokens_np)"
      ],
      "metadata": {
        "id": "Fsx7eYrjb7r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = os.path.join(local_dir, f\"edufineweb_\")\n",
        "filename"
      ],
      "metadata": {
        "id": "xLkbC9qZcHFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mp.Pool(nprocs) as pool:\n",
        "    shard_index = 0\n",
        "    # preallocate buffer to hold current shard\n",
        "    all_tokens_np = np.empty((shard_size,), dtype=np.uint16)\n",
        "    token_count = 0\n",
        "    progress_bar = None\n",
        "    for tokens in pool.imap(tokenize, fw, chunksize=16):\n",
        "\n",
        "        # is there enough space in the current shard for the new tokens?\n",
        "        if token_count + len(tokens) < shard_size:\n",
        "            # simply append tokens to current shard\n",
        "            all_tokens_np[token_count:token_count+len(tokens)] = tokens\n",
        "            token_count += len(tokens)\n",
        "            # update progress bar\n",
        "            #if progress_bar is None:\n",
        "            #    progress_bar = tqdm(total=shard_size, unit=\"tokens\", desc=f\"Shard {shard_index}\")\n",
        "            #progress_bar.update(len(tokens))\n",
        "        else:\n",
        "            # write the current shard and start a new one\n",
        "            split = \"val\" if shard_index == 0 else \"train\"\n",
        "            filename = os.path.join(local_dir, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "            # split the document into whatever fits in this shard; the remainder goes to next one\n",
        "            remainder = shard_size - token_count\n",
        "            #progress_bar.update(remainder)\n",
        "            all_tokens_np[token_count:token_count+remainder] = tokens[:remainder]\n",
        "            print(filename)\n",
        "            write_datafile(filename, all_tokens_np)\n",
        "            shard_index += 1\n",
        "            progress_bar = None\n",
        "            # populate the next shard with the leftovers of the current doc\n",
        "            all_tokens_np[0:len(tokens)-remainder] = tokens[remainder:]\n",
        "            token_count = len(tokens)-remainder\n",
        "\n",
        "    # write any remaining tokens as the last shard\n",
        "    if token_count != 0:\n",
        "        split = \"val\" if shard_index == 0 else \"train\"\n",
        "        filename = os.path.join(local_dir, f\"edufineweb_{split}_{shard_index:06d}\")\n",
        "        write_datafile(filename, all_tokens_np[:token_count])"
      ],
      "metadata": {
        "id": "EwkKiCj5cHSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload files to S3"
      ],
      "metadata": {
        "id": "DTsrL-wtcPVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = boto3.client('s3', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
        "bucket_name = \"fineweb-10b-tokenized-071024\"\n"
      ],
      "metadata": {
        "id": "ORGxVbPKcHV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upload_file_to_s3(file_path, bucket_name):\n",
        "  try:\n",
        "      s3.upload_file(file_path, bucket_name, file_path)\n",
        "      print(f\"File {file_path} successfully uploaded to S3 bucket {bucket_name}.\")\n",
        "  except Exception as e:\n",
        "      print(f\"Error uploading file to S3: {e}\")"
      ],
      "metadata": {
        "id": "JyfDRPKZcHdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Specify the directory path\n",
        "directory_path = \"edu_fineweb10B\"\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(directory_path)\n",
        "\n",
        "# Print the list of files\n",
        "print(\"Files in the directory:\")\n",
        "for file in files:\n",
        "  file_path = os.path.join(directory_path, file)\n",
        "  print(file_path)\n",
        "  upload_file_to_s3(file_path, bucket_name)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cyRnb6AocS4p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}